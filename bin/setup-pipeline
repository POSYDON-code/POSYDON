#!/usr/bin/env python
__authors__ = [
    "Simone Bavera <Simone.Bavera@unige.ch>",
]

import os
import pandas as pd
import numpy as np
from posydon.utils.common_functions import PATH_TO_POSYDON

# this data processing pipeline was designed assuming POSYDON v2 data structure
VERSION = '' # quest: 'v2' yggdrasil: ''
'''
Data tree structure

PATH_TO_GRIDS/
    /HMS-HMS/
    /CO-HMS_RLO/
    /CO-HeMS/
    /single_HMS/
    /single_HeMS/
        /v1/
        /v2/
            /1e+00_Zsun/
            /1e-01_Zsun/
            /1e-02_Zsun/
            ...
                /grid_low_res_0/
                /grid_low_res_1/
                /grid_rerun_1/
                ...
                /LITE/
                /ORIGINAL/
                /logs/
                /scripts/
                /plots/
                    /grid_low_res_combined/
                        /TF1/
                        /TF2/
                        ...
'''

def slurm_job(job_name,
              CREATE_GRID_SLICES=False,
              COMBINE_GRID_SLICES=False,
              PLOT_GRIDS=False,
              CHECK_FAILURE_RATE=False,
              POST_PROCESSING=False,
              TRAIN_INTERPOLATORS=False,
              verbose=False,
              ):

    path_to_csv_file = os.path.join(PATH,f"{job_name}.csv")

    with open(f'{job_name}.slurm', 'w') as f:
        f.write("#!/bin/bash\n")
        f.write(f"#SBATCH --account={ACCOUNT}\n")
        f.write(f"#SBATCH --partition={PARTITION}\n")
        f.write("#SBATCH -N 1\n")
        f.write("#SBATCH --cpus-per-task 1\n")
        f.write("#SBATCH --ntasks-per-node 1\n")
        f.write(f"#SBATCH --time={WALLTIME}\n")
        f.write("#SBATCH --job-name=psygrid\n")
        f.write("#SBATCH --mem-per-cpu=4G\n")

        if EMAIL is not None:
            f.write(f"#SBATCH --mail-type={MAILTYPE}\n")
            f.write(f"#SBATCH --mail-user={EMAIL}\n")

        if (CREATE_GRID_SLICES or PLOT_GRIDS or CHECK_FAILURE_RATE or
            POST_PROCESSING or TRAIN_INTERPOLATORS):
            df = pd.read_csv(path_to_csv_file)
            N = df.shape[0]-1
            f.write(f"#SBATCH --array=0-{N}\n")
            slurm_array = '$SLURM_ARRAY_TASK_ID'
        else:
            slurm_array = ''

        if CREATE_GRID_SLICES:
            f.write(f"#SBATCH --output={PATH}/logs/grid_slice_%a.out\n")

        if COMBINE_GRID_SLICES:
            f.write(f"#SBATCH --output={PATH}/logs/combine_grid_slices.out\n")

        if PLOT_GRIDS:
            f.write(f"#SBATCH --output={PATH}/logs/plot_grid_%a.out\n")

        if CHECK_FAILURE_RATE:
            f.write(f"#SBATCH --output={PATH}/logs/check_failure_rate_%a.out\n")

        if POST_PROCESSING:
            f.write(f"#SBATCH --output={PATH}/logs/post_processing_%a.out\n")
            f.write(f"export PATH_TO_POSYDON={PATH_TO_POSYDON}\n")

        if TRAIN_INTERPOLATORS:
            f.write(f"#SBATCH --output={PATH}/logs/train_interpolators_%a.out\n")

        f.write(f"\nsrun python {PATH_TO_POSYDON}/bin/run-pipeline {path_to_csv_file} {slurm_array}")

# create csv file with a list of all grid paths to process
def create_csv(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS,
               step_name=None, version=VERSION, GRIDS_COMBINED=None,
               INTERPOLATION_METHODS=None):

    # number of grid types
    N = len(GRID_TYPES)
    if ( N != len(METALLICITIES) or N != len(GRID_SLICES) or
         N != len(COMPRESSIONS)):
        raise ValueError('Missmatch between the len of GRID_TYPES, '
                         'METALLICITIES, GRID_SLICES, COMPRESSIONS.')

    grids = []
    grids_compression = []
    plot_dirs = []
    processed_grids = []
    interpolators = []
    df = pd.DataFrame()
    for l, grid_type in enumerate(GRID_TYPES):

        METALLICITIES_ = METALLICITIES[l]
        GRID_SLICES_ = GRID_SLICES[l]
        COMPRESSIONS_ = COMPRESSIONS[l]

        for metallicity in METALLICITIES_:
            for i, grid_slice in enumerate(GRID_SLICES_):
                for compression in COMPRESSIONS_:
                    if step_name == 'step_1':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, version, metallicity, grid_slice))
                        grids_compression.extend([compression])
                    elif step_name == 'step_2':
                        if N != len(GRIDS_COMBINED):
                            raise ValueError('len(GRID_TYPES) != len(GRIDS_COMBINED)!')
                        if len(GRID_SLICES[l]) != len(GRIDS_COMBINED[l]):
                            raise ValueError('len(GRID_SLICES[l]) != len(GRIDS_COMBINED[l])!')
                        combine_grid_slices = []
                        # grid_slice is a batch
                        for grid_slice_ in grid_slice:
                                path_to_grid = os.path.join(PATH_TO_GRIDS,
                                              grid_type, version, metallicity,
                                              compression, grid_slice_+'.h5')
                                combine_grid_slices.append(path_to_grid)
                        path_to_grid_combined = os.path.join(PATH_TO_GRIDS,
                                                             grid_type, version,
                                                             metallicity,
                                                             compression,
                                                             GRIDS_COMBINED[l][i]+'.h5')
                        df_tmp = pd.DataFrame()
                        df_tmp[path_to_grid_combined] = combine_grid_slices
                        df = pd.concat([df,df_tmp], axis=1)
                    elif step_name in ['step_3', 'step_4']:
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, version, metallicity,
                                      compression, grid_slice+'.h5'))
                        plot_dirs.append(os.path.join(PATH_TO_GRIDS,
                                    grid_type, version, metallicity,
                                    'plots', grid_slice))
                    elif step_name == 'step_5':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, version, metallicity,
                                      compression, grid_slice+'.h5'))
                        processed_grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, version, metallicity,
                                      compression, grid_slice+'_processed.h5'))
                    elif step_name == 'step_6':
                        for method in INTERPOLATION_METHODS:
                            grids.append(os.path.join(PATH_TO_GRIDS,
                                          grid_type, version, metallicity,
                                          compression, grid_slice+'.h5'))
                            interpolators.append(os.path.join(PATH_TO_GRIDS,
                                          grid_type, version, metallicity,
                                          'interpolation_objects',
                                          'IF_'+method+'.pkl'))

    # saving dataset to csv file
    if step_name != 'step_2':
        grids = np.array(grids)
        df['path_to_grid'] = grids
        if step_name == 'step_1':
            df['compression'] = grids_compression
        elif step_name in ['step_3', 'step_4']:
            df['path_to_plot'] = plot_dirs
        elif step_name == 'step_5':
            df['path_to_processed_grid'] = processed_grids
        elif step_name == 'step_6':
            df['path_to_interpolator'] = interpolators
    output_fname = f'{step_name}.csv'
    df.to_csv(os.path.join(PATH, output_fname), index=False)

if __name__ == '__main__':

    EXAMPLE = 'yggdrasil'

    # run a test on quest, currently only HMS-HMS grids are available
    if EXAMPLE == 'quest':

        PATH_TO_GRIDS = '/projects/b1119/POSYDON_GRIDS/'
        PATH = '.' # working dir
        ACCOUNT = 'b1119'
        PARTITION = 'posydon-priority'
        WALLTIME = '24:00:00'
        MAILTYPE = 'FAIL'
        EMAIL = 'simone.bavera@unige.ch'
        VERBOSE = True

        # EXAMPLE SETUP (they currently run independently)
        # TODO: add a linked slurm job arrays which are waiting for each others
        # STEPS 1 to 6
        CREATE_GRID_SLICES = True
        COMBINE_GRID_SLICES = True
        PLOT_GRIDS = True
        CHECK_FAILURE_RATE = True
        POST_PROCESSING = True
        TRAIN_INTERPOLATORS = True

        ###### EXAMPLE step 1 ######
        GRID_TYPES_s1 = ['HMS-HMS']
        METALLICITIES_s1 = [['1e-01_Zsun']]
        GRID_SLICES_s1 = [# HMS-HMS
                          ['grid_low_res_0','grid_low_res_1','grid_low_res_2',
                           'grid_low_res_3','grid_low_res_4','grid_low_res_5',
                           'grid_low_res_rerun_opacitymax']
                          ]
        COMPRESSIONS_s1 = [['LITE','ORIGINAL']]

        ###### EXAMPLE step 2 ######
        GRID_TYPES_s2 = ['HMS-HMS']
        METALLICITIES_s2 = [['1e-01_Zsun']]
        GRID_SLICES_s2 = [# HMS-HMS
                          [['grid_low_res_0','grid_low_res_1','grid_low_res_2',
                            'grid_low_res_3','grid_low_res_4','grid_low_res_5'],
                           ['grid_low_res_combined','grid_low_res_rerun_opacitymax']]
                          ]
        GRIDS_COMBINED_s2 = [# HMS-HMS
                             ['grid_low_res_combined','grid_low_res_combined_rerun_1']
                             ]
        COMPRESSIONS_s2 = [['LITE','ORIGINAL']]

        ###### EXAMPLE step 3 ######
        GRID_TYPES_s3 = ['HMS-HMS']
        METALLICITIES_s3 = [['1e-01_Zsun']]
        GRID_SLICES_s3 = [['grid_low_res_combined','grid_low_res_rerun_opacitymax',
                       'grid_low_res_combined_rerun_1']]
        COMPRESSIONS_s3 = [['LITE']]

        ###### EXAMPLE step 4 ######
        GRID_TYPES_s4 = ['HMS-HMS']
        METALLICITIES_s4 = [['1e-01_Zsun']]
        GRID_SLICES_s4 = [['grid_low_res_combined','grid_low_res_rerun_opacitymax',
                       'grid_low_res_combined_rerun_1']]
        COMPRESSIONS_s4 = [['LITE']]

        ###### EXAMPLE step 5 ######
        GRID_TYPES_s5 = ['HMS-HMS']
        METALLICITIES_s5 = [['1e-01_Zsun']]
        GRID_SLICES_s5 = [['grid_low_res_combined_rerun_1']]
        COMPRESSIONS_s5 = [['LITE']]

        ###### EXAMPLE step 6 ######
        GRID_TYPES_s6 = ['HMS-HMS']
        METALLICITIES_s6 = [['1e-01_Zsun']]
        GRID_SLICES_s6 = [['grid_low_res_combined_rerun_1_processed']]
        INTERPOLATION_METHODS_s6 = ["linear","1NN"]
        COMPRESSIONS_s6 = [['LITE']]

    elif EXAMPLE == 'yggdrasil':

        PATH_TO_GRIDS = '/srv/beegfs/scratch/shares/astro/posydon/POSYDON_GRIDS_v2/'
        PATH = '.' # working dir
        ACCOUNT = 'meynet'
        PARTITION = 'shared-cpu'
        WALLTIME = '08:00:00'
        MAILTYPE = 'FAIL'
        EMAIL = 'simone.bavera@unige.ch'
        VERBOSE = True

        # EXAMPLE SETUP (they currently run independently)
        # TODO: add a linked slurm job arrays which are waiting for each others
        # STEPS 1 to 5
        CREATE_GRID_SLICES = True
        COMBINE_GRID_SLICES = True
        PLOT_GRIDS = True
        POST_PROCESSING = True
        TRAIN_INTERPOLATORS = True
        # OPTIONL STEPS
        CHECK_FAILURE_RATE = True

        ###### EXAMPLE step 1 ######
        GRID_TYPES_s1 = ['CO-HMS_RLO','CO-HeMS','HMS-HMS']
        METALLICITIES_s1 = [['1e-01_Zsun'],['1e-01_Zsun'],['1e-04_Zsun']]
        GRID_SLICES_s1 = [# CO-HMS
                          ['grid_low_res_0','grid_low_res_1','grid_low_res_2',
                           'grid_random_1'],
                          # CO-HeMS
                          ['grid_low_res_0','grid_low_res_1','grid_low_res_2',
                           'grid_random_1']
                          # HMS-HMS
                          ['grid_low_res_3','grid_low_res_4','grid_low_res_5',
                           'grid_random_1']
                          ]
        COMPRESSIONS_s1 = [['LITE','ORIGINAL'],['LITE','ORIGINAL'],['LITE','ORIGINAL']]

        ###### EXAMPLE step 2 ######
        GRID_TYPES_s2 = ['CO-HMS_RLO','CO-HeMS']
        METALLICITIES_s2 = [['1e-01_Zsun'],['1e-01_Zsun']]
        GRID_SLICES_s2 = [# CO-HMS
                          [['grid_low_res_0','grid_low_res_1','grid_low_res_2']],
                          # CO-HeMS
                          [['grid_low_res_0','grid_low_res_1','grid_low_res_2']],
                          ]
        GRIDS_COMBINED_s2 = [# CO-HMS
                             ['grid_low_res_combined'],
                             # CO-HeMS
                             ['grid_low_res_combined']
                             ]
        COMPRESSIONS_s2 = [['LITE','ORIGINAL'],['LITE','ORIGINAL']]

        ###### EXAMPLE step 3 ######
        GRID_TYPES_s3 = ['CO-HMS_RLO','CO-HeMS']
        METALLICITIES_s3 = [['1e-01_Zsun'],['1e-01_Zsun']]
        GRID_SLICES_s3 = [['grid_low_res_combined'],['grid_low_res_combined']]
        COMPRESSIONS_s3 = [['LITE'],['LITE']]

        ###### EXAMPLE step 4 ######
        GRID_TYPES_s4 = ['CO-HMS_RLO','CO-HeMS']
        METALLICITIES_s4 = [['1e-01_Zsun'],['1e-01_Zsun']]
        GRID_SLICES_s4 = [['grid_low_res_combined'],['grid_low_res_combined']]
        COMPRESSIONS_s4 = [['LITE'],['LITE']]

        ###### EXAMPLE step 5 ######
        GRID_TYPES_s5 = ['CO-HMS_RLO','CO-HeMS']
        METALLICITIES_s5 = [['1e-01_Zsun'],['1e-01_Zsun']]
        GRID_SLICES_s5 = [['grid_low_res_combined'],
                          ['grid_low_res_combined']]
        COMPRESSIONS_s5 = [['LITE'],['LITE']]

        ###### EXAMPLE step 6 ######
        GRID_TYPES_s6 = ['CO-HMS_RLO','CO-HeMS']
        METALLICITIES_s6 = [['1e-01_Zsun'],['1e-01_Zsun']]
        GRID_SLICES_s6 = [['grid_low_res_combined_processed'],
                          ['grid_low_res_combined_processed']]
        INTERPOLATION_METHODS_s6 = ["linear","1NN"]
        COMPRESSIONS_s6 = [['LITE'],['LITE']]

    if CREATE_GRID_SLICES:
        create_csv(GRID_TYPES_s1, METALLICITIES_s1, GRID_SLICES_s1,
                   COMPRESSIONS_s1, step_name='step_1')
        slurm_job(job_name='step_1',
                  CREATE_GRID_SLICES=CREATE_GRID_SLICES,
                  verbose=VERBOSE)

    if COMBINE_GRID_SLICES:
        create_csv(GRID_TYPES_s2, METALLICITIES_s2, GRID_SLICES_s2, COMPRESSIONS_s2,
                   step_name='step_2', GRIDS_COMBINED=GRIDS_COMBINED_s2)
        slurm_job(job_name='step_2',
                  COMBINE_GRID_SLICES=COMBINE_GRID_SLICES,
                  verbose=VERBOSE)

    if PLOT_GRIDS:
        create_csv(GRID_TYPES_s3, METALLICITIES_s3, GRID_SLICES_s3,
                   COMPRESSIONS_s3, step_name='step_3')
        slurm_job(job_name='step_3',
                  PLOT_GRIDS=PLOT_GRIDS,
                  verbose=VERBOSE)

    if CHECK_FAILURE_RATE:
        create_csv(GRID_TYPES_s4, METALLICITIES_s4, GRID_SLICES_s4,
                   COMPRESSIONS_s4, step_name='step_4')
        slurm_job(job_name='step_4',
                  CHECK_FAILURE_RATE=CHECK_FAILURE_RATE,
                  verbose=VERBOSE)

    if POST_PROCESSING:
        create_csv(GRID_TYPES_s5, METALLICITIES_s5, GRID_SLICES_s5,
                   COMPRESSIONS_s5, step_name='step_5')
        slurm_job(job_name='step_5',
                  POST_PROCESSING=POST_PROCESSING,
                  verbose=VERBOSE)

    if TRAIN_INTERPOLATORS:
        create_csv(GRID_TYPES_s6, METALLICITIES_s6, GRID_SLICES_s6,
                   COMPRESSIONS_s6, step_name='step_6',
                   INTERPOLATION_METHODS=INTERPOLATION_METHODS_s6)
        slurm_job(job_name='step_6',
                  TRAIN_INTERPOLATORS=TRAIN_INTERPOLATORS,
                  verbose=VERBOSE)

    # TODO: export final datasets for v2

    # TODO: SETUP RERUNS

    # create logs in working dir to store all slurm outputs
    logs_path = os.path.join(PATH,'logs')
    if not os.path.isdir(logs_path):
        os.makedirs(logs_path)

    # TODO: check that LITE/ ORIGINAL/ plots/ are present in all directories
