#!/usr/bin/env python
__authors__ = [
    "Simone Bavera <Simone.Bavera@unige.ch>",
]

import os
import pandas as pd
import numpy as np
from posydon.utils.common_functions import PATH_TO_POSYDON

# this data processing pipeline was designed assuming POSYDON v2 data structure
VERSION = 'v2'
'''
Data tree structure

PATH_TO_GRIDS/
    /HMS-HMS/
    /CO-HMS_RLO/
    /CO-HeMS/
    /single_HMS/
    /single_HeMS/
        /v1/
        /v2/
            /1e+00_Zsun/
            /1e-01_Zsun/
            /1e-02_Zsun/
            ...
                /grid_low_res_0/
                /grid_low_res_1/
                /grid_rerun_1/
                ...
                /LITE/
                /ORIGINAL/
                /logs/
                /scripts/
                /plots/
                    /grid_low_res_combined/
                        /TF1/
                        /TF2/
                        ...
'''

def slurm_job(job_name,
              CREATE_GRID_SLICES=False,
              COMBINE_GRID_SLICES=False,
              PLOT_GRIDS=False,
              CHECK_FAILURE_RATE=False,
              POST_PROCESSING=False,
              TRAIN_INTERPOLATORS=False,
              verbose=False,
              ):

    path_to_csv_file = os.path.join(PATH,f"{job_name}.csv")

    with open(f'{job_name}.slurm', 'w') as f:
        f.write("#!/bin/bash\n")
        f.write("#SBATCH --account=b1119\n")
        f.write("#SBATCH --partition=posydon-priority\n")
        f.write("#SBATCH -N 1\n")
        f.write("#SBATCH --cpus-per-task 1\n")
        f.write("#SBATCH --ntasks-per-node 1\n")
        f.write("#SBATCH --time=24:00:00\n")
        f.write("#SBATCH --job-name=psygrid\n")
        f.write("#SBATCH --mem-per-cpu=4G\n")

        if EMAIL is not None:
            f.write("#SBATCH --mail-type=ALL\n")
            f.write(f"#SBATCH --mail-user={EMAIL}\n")

        if (CREATE_GRID_SLICES or PLOT_GRIDS or CHECK_FAILURE_RATE or
            POST_PROCESSING or TRAIN_INTERPOLATORS):
            df = pd.read_csv(path_to_csv_file)
            N = df.shape[0]-1
            f.write(f"#SBATCH --array=0-{N}\n")

        if CREATE_GRID_SLICES:
            f.write(f"#SBATCH --output={PATH}/logs/grid_slice_%a.out\n")

        if COMBINE_GRID_SLICES:
            f.write(f"#SBATCH --output={PATH}/logs/combine_grid_slices.out\n")

        if PLOT_GRIDS:
            f.write(f"#SBATCH --output={PATH}/logs/plot_grid_%a.out\n")

        if CHECK_FAILURE_RATE:
            f.write(f"#SBATCH --output={PATH}/logs/check_failure_rate_%a.out\n")

        if POST_PROCESSING:
            f.write(f"#SBATCH --output={PATH}/logs/post_processing_%a.out\n")

        if TRAIN_INTERPOLATORS:
            f.write(f"#SBATCH --output={PATH}/logs/post_processing_%a.out\n")
            f.write(f"export PATH_TO_POSYDON={PATH_TO_POSYDON}\n")

        f.write(f"\nsrun python {PATH_TO_POSYDON}/bin/run-pipeline {path_to_csv_file} $SLURM_ARRAY_TASK_ID")

# create csv file with a list of all grid paths to process
def create_csv_step_1(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS,
                      output_file='step_1.csv', version=VERSION):

    grids = []
    for grid_type in GRID_TYPES:
        for metallicity in METALLICITIES:
            for grid_slice in GRID_SLICES:
                for compression in COMPRESSIONS:
                    grids.append([os.path.join(PATH_TO_GRIDS,
                                  grid_type, version, metallicity, grid_slice),
                                  compression])
    grids = np.array(grids)
    df = pd.DataFrame()
    df['path_to_grid'] = grids[:,0]
    df['compression'] = grids[:,1]
    df.to_csv(os.path.join(PATH, output_file), index=False)

def create_csv_step_2(GRID_TYPES, METALLICITIES, GRID_SLICES,
                      GRIDS_COMBINED, COMPRESSIONS,
                      output_file='step_2.csv', version=VERSION):

    if len(GRID_SLICES) != len(GRIDS_COMBINED):
        raise ValueError('len(GRID_SLICES) =! len(GRIDS_COMBINED)!')

    df = pd.DataFrame()
    for grid_type in GRID_TYPES:
        for metallicity in METALLICITIES:
            for i, grid_slice_batch in enumerate(GRID_SLICES):
                for k, compression in enumerate(COMPRESSIONS):
                    combine_grid_slices = []
                    for grid_slice in grid_slice_batch:
                            path_to_grid = os.path.join(PATH_TO_GRIDS,
                                          grid_type, version, metallicity,
                                          compression, grid_slice+'.h5')
                            combine_grid_slices.append(path_to_grid)
                    path_to_grid_combined = os.path.join(PATH_TO_GRIDS,
                                                         grid_type, version,
                                                         metallicity,
                                                         compression,
                                                         GRIDS_COMBINED[i]+'.h5')
                    df_tmp = pd.DataFrame()
                    df_tmp[path_to_grid_combined] = combine_grid_slices
                    df = pd.concat([df,df_tmp], axis=1)
    df.to_csv(os.path.join(PATH, output_file), index=False)

def create_csv_step_3(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS,
                      output_file='step_3.csv', version=VERSION):

    grids = []
    plot_dirs = []
    for grid_type in GRID_TYPES:
        for metallicity in METALLICITIES:
            for grid_slice in GRID_SLICES:
                for compression in COMPRESSIONS:
                    grids.append(os.path.join(PATH_TO_GRIDS,
                                  grid_type, version, metallicity,
                                  compression, grid_slice+'.h5'))
                    plot_dirs.append(os.path.join(PATH_TO_GRIDS,
                                grid_type, version, metallicity,
                                'plots', grid_slice))
    grids = np.array(grids)
    df = pd.DataFrame()
    df['path_to_grid'] = grids
    df['path_to_plot'] = plot_dirs
    df.to_csv(os.path.join(PATH, output_file), index=False)

def create_csv_step_4(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS,
                      output_file='step_4.csv', version=VERSION):

    grids = []
    processed_grids = []
    for grid_type in GRID_TYPES:
        for metallicity in METALLICITIES:
            for grid_slice in GRID_SLICES:
                for compression in COMPRESSIONS:
                    grids.append(os.path.join(PATH_TO_GRIDS,
                                  grid_type, version, metallicity,
                                  compression, grid_slice+'.h5'))
                    processed_grids.append(os.path.join(PATH_TO_GRIDS,
                                  grid_type, version, metallicity,
                                  compression, grid_slice+'_processed.h5'))
    grids = np.array(grids)
    df = pd.DataFrame()
    df['path_to_grid'] = grids
    df['path_to_processed_grid'] = processed_grids
    df.to_csv(os.path.join(PATH, output_file), index=False)

def create_csv_step_5(GRID_TYPES, METALLICITIES, GRID_SLICES,
                      INTERPOLATION_METHODS, COMPRESSIONS,
                      output_file='step_5.csv', version=VERSION):

    grids = []
    interpolators = []
    for grid_type in GRID_TYPES:
        for metallicity in METALLICITIES:
            for grid_slice in GRID_SLICES:
                for method in INTERPOLATION_METHODS:
                    for compression in COMPRESSIONS:
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, version, metallicity,
                                      compression, grid_slice+'.h5'))
                        interpolators.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, version, metallicity,
                                      'interpolation_objects',
                                      method+'.pkl'))
    grids = np.array(grids)
    df = pd.DataFrame()
    df['path_to_grid'] = grids
    df['path_to_interpolator'] = interpolators
    df.to_csv(os.path.join(PATH,output_file), index=False)

if __name__ == '__main__':

    PATH_TO_GRIDS = '/projects/b1119/POSYDON_GRIDS/'
    PATH = '.' # working dir
    EMAIL = 'simone.bavera@unige.ch'
    VERBOSE = True

    # EXAMPLE SETUP (they currently run independently)
    # TODO: add a linked slurm job arrays which are waiting for each others
    CREATE_GRID_SLICES = True
    COMBINE_GRID_SLICES = True
    PLOT_GRIDS = True
    CHECK_FAILURE_RATE = True
    POST_PROCESSING = True
    TRAIN_INTERPOLATORS = True

    if CREATE_GRID_SLICES:
        ###### EXAMPLE ######
        GRID_TYPES = ['HMS-HMS']
        METALLICITIES = ['1e-01_Zsun']
        GRID_SLICES = ['grid_low_res_0','grid_low_res_1','grid_low_res_2',
                        'grid_low_res_3','grid_low_res_4','grid_low_res_5',
                        'grid_rerun_opacitymax']
        COMPRESSIONS = ['LITE','ORIGINAL']
        #####################
        create_csv_step_1(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS)

        # create the slurm job
        slurm_job(job_name='step_1',
                  CREATE_GRID_SLICES=CREATE_GRID_SLICES,
                  verbose=VERBOSE)

    if COMBINE_GRID_SLICES:
        ###### EXAMPLE ######
        GRID_TYPES = ['HMS-HMS']
        METALLICITIES = ['1e-01_Zsun']
        GRID_SLICES = [['grid_low_res_0','grid_low_res_1','grid_low_res_2',
                        'grid_low_res_3','grid_low_res_4','grid_low_res_5'],
                        ['grid_low_res_combined','grid_low_res_rerun_opacitymax'],
                        ]
        GRIDS_COMBINED = ['grid_low_res_combined','grid_low_res_combined_rerun_1']
        COMPRESSIONS = ['LITE','ORIGINAL']
        #####################
        create_csv_step_2(GRID_TYPES, METALLICITIES, GRID_SLICES,
                          GRIDS_COMBINED, COMPRESSIONS)

        # create the slurm job
        slurm_job(job_name='step_2',
                  COMBINE_GRID_SLICES=COMBINE_GRID_SLICES,
                  verbose=VERBOSE)

    if PLOT_GRIDS:
        ###### EXAMPLE ######
        GRID_TYPES = ['HMS-HMS']
        METALLICITIES = ['1e-01_Zsun']
        GRID_SLICES = ['grid_low_res_combined','grid_low_res_rerun_opacitymax',
                       'grid_low_res_combined_rerun_1']
        COMPRESSIONS = ['LITE']
        #####################
        create_csv_step_3(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS)

        # create the slurm job
        slurm_job(job_name='step_3',
                  PLOT_GRIDS=PLOT_GRIDS,
                  verbose=VERBOSE)

    if POST_PROCESSING:
        ###### EXAMPLE ######
        GRID_TYPES = ['HMS-HMS']
        METALLICITIES = ['1e-01_Zsun']
        GRID_SLICES = ['grid_low_res_combined_rerun_1']
        COMPRESSIONS = ['LITE']
        #####################
        create_csv_step_4(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS)

        # create the slurm job
        slurm_job(job_name='step_4',
                  POST_PROCESSING=POST_PROCESSING,
                  verbose=VERBOSE)

    if TRAIN_INTERPOLATORS:
        ###### EXAMPLE ######
        GRID_TYPES = ['HMS-HMS']
        METALLICITIES = ['1e-01_Zsun']
        GRID_SLICES = ['grid_low_res_combined_rerun_1']
        INTERPOLATION_METHODS = ["linear","1NN"]
        COMPRESSIONS = ['LITE']
        #####################
        create_csv_step_5(GRID_TYPES, METALLICITIES, GRID_SLICES,
                          INTERPOLATION_METHODS, COMPRESSIONS)

        # create the slurm job
        slurm_job(job_name='step_5',
                  TRAIN_INTERPOLATORS=TRAIN_INTERPOLATORS,
                  verbose=VERBOSE)

    if CHECK_FAILURE_RATE:
        ###### EXAMPLE ######
        GRID_TYPES = ['HMS-HMS']
        METALLICITIES = ['1e-01_Zsun']
        GRID_SLICES = ['grid_low_res_combined','grid_low_res_rerun_opacitymax',
                       'grid_low_res_combined_rerun_1']
        COMPRESSIONS = ['LITE']
        #####################
        create_csv_step_3(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS,
                          output_file='step_optional_1.csv')

        # create the slurm job
        slurm_job(job_name='step_optional_1',
                  CHECK_FAILURE_RATE=CHECK_FAILURE_RATE,
                  verbose=VERBOSE)

    # TODO: SETUP RERUNS

    # TODO: export final datasets for v2

    # create logs in working dir to store all slurm outputs
    logs_path = os.path.join(PATH,'logs')
    if not os.path.isdir(logs_path):
        os.makedirs(logs_path)

    # TODO: check that LITE/ ORIGINAL/ plots/ are present in all directories
