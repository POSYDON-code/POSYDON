#!/usr/bin/env python
__authors__ = [
    "Simone Bavera <Simone.Bavera@unige.ch>",
    "Kyle Akira Rocha <kylerocha2024@u.northwestern.edu>",
    "Matthias Kruckow <Matthias.Kruckow@unige.ch>",
]

import os
import sys
import ast
import shutil
import pandas as pd
import numpy as np
from pprint import pprint, pformat
from posydon.popsyn.io import parse_inifile
from posydon.utils.common_functions import PATH_TO_POSYDON
from posydon.utils.gridutils import get_new_grid_name

# this data processing pipeline was designed assuming POSYDON v2 data structure
'''
Data tree structure

PATH_TO_GRIDS/
    /HMS-HMS/
    /CO-HMS_RLO/
    /CO-HeMS/
    /single_HMS/
    /single_HeMS/
        /v1/
        /v2/
            /1e+00_Zsun/
            /1e-01_Zsun/
            /1e-02_Zsun/
            ...
                /grid_low_res_0/
                /grid_low_res_1/
                /grid_rerun_1/
                ...
                /LITE/
                /ORIGINAL/
                /logs/
                /scripts/
                /plots/
                    /grid_low_res_combined/
                        /TF1/
                        /TF2/
                        ...
'''

ACTION_TO_STEP_NUM = {
    'CREATE_GRID_SLICES'  : 'step_1',
    'COMBINE_GRID_SLICES' : 'step_2',
    'PLOT_GRIDS'          : 'step_3',
    'CHECK_FAILURE_RATE'  : 'step_4',
    'POST_PROCESSING'     : 'step_5',
    'TRAIN_INTERPOLATORS' : 'step_6',
    'EXPORT_DATASET'      : 'step_7',
    'RERUN'               : 'rerun'
}

class PostProcessingPipeline:

    def __init__(self, path_to_inifile=None):
        self.PATH_TO_POSYDON = os.getenv('PATH_TO_POSYDON')
        self.PATH_TO_INIFILE = path_to_inifile

        if self.PATH_TO_INIFILE is not None:
            self.pipeline_kwargs = self.parse_setup_params(self.PATH_TO_INIFILE)


    @staticmethod
    def parse_setup_params(path=None):
        """
        Parse inifile for running post-processing pipelines.
        """

        if path is None:
            return
        else:
            parser = parse_inifile(path)

        pipeline_kwargs = dict()
        for section in parser.sections():
            section_dict = dict()
            for key, val in parser[section].items():
                section_dict[key] = ast.literal_eval(val)

            pipeline_kwargs[section] = section_dict
        return pipeline_kwargs

    def create_csv_and_slurm_job_files(self):
        previously_created_files = []
        setup_kwargs = self.pipeline_kwargs['pipeline setup']
        account_kwargs = self.pipeline_kwargs['account']

        if setup_kwargs['VERBOSE']:
            print( "\n\n{:+^45s} \n{}\n{:+^45s}\n{}".format(
                    'ACCOUNT', pformat(account_kwargs, indent=2),
                    'SETUP', pformat(setup_kwargs, indent=2) )
                 )


        with open('run_pipeline.sh', 'w') as runfile:
            runfile.write("#!/bin/bash\n")
            last_step = ''
            for action, step_number in ACTION_TO_STEP_NUM.items():
                do_step_bool = setup_kwargs[action]

                if setup_kwargs['VERBOSE']:
                    print( "\n\n{:-^45s} {:^8s}:{:^6s}".format(
                        action, step_number, str(do_step_bool)) )

                if do_step_bool:
                    step_kwargs = self.pipeline_kwargs[step_number]

                    if setup_kwargs['VERBOSE']:
                        print( pformat(step_kwargs, indent=2) )


                    previously_created_files += create_csv(
                        step_name=step_number,
                        previously_created_files=previously_created_files,
                        **{**step_kwargs, **setup_kwargs}
                    )
                    slurm_job(
                        job_name=step_number,
                        step_name=step_number,
                        PATH_TO_POSYDON=self.PATH_TO_POSYDON,
                        **{**step_kwargs, **setup_kwargs, **account_kwargs}
                    )
                    if last_step == '':
                        # if there was no previous step the current step has no dependency
                        runfile.write(f"ID{step_number}=$(sbatch --parsable {step_number}.slurm)\n")
                    else:
                        if setup_kwargs['COMBINE_GRID_SLICES'] and (step_number == 'step_4' or step_number == 'step_5'):
                            # steps which can run in paralell after step_2 (steps 4 and 5 can run parallel with step_3)
                            runfile.write(f"ID{step_number}=$(sbatch --parsable --dependency=afterok:$"
                                          "{IDstep_2}"f" --kill-on-invalid-dep=yes {step_number}.slurm)\n")
                        elif setup_kwargs['CREATE_GRID_SLICES'] and (step_number == 'step_4' or step_number == 'step_5'):
                            # steps which can run in paralell after step_1 (in case there is no step_2)
                            runfile.write(f"ID{step_number}=$(sbatch --parsable --dependency=afterok:$"
                                          "{IDstep_1}"f" --kill-on-invalid-dep=yes {step_number}.slurm)\n")
                        else:
                            # steps waiting for the previous step
                            runfile.write(f"ID{step_number}=$(sbatch --parsable --dependency=afterok:$"
                                          "{"f"ID{last_step}""}"f" --kill-on-invalid-dep=yes {step_number}.slurm)\n")
                    runfile.write(f"echo '{step_number}.slurm submitted as '$"
                                  "{"f"ID{step_number}""}"f"\n")
                    last_step = step_number
        os.system("chmod 755 run_pipeline.sh")

    def create_logs_dir(self):
        # create logs in working dir to store all slurm outputs
        setup_kwargs = self.pipeline_kwargs['pipeline setup']
        logs_path = os.path.join(setup_kwargs['PATH'],'logs')
        if not os.path.isdir(logs_path):
            os.makedirs(logs_path)

    def create_export_dir(self):
        # create data dir three to export the datasets
        setup_kwargs = self.pipeline_kwargs['pipeline setup']
        if setup_kwargs['EXPORT_DATASET']:
            data_path = os.path.join(setup_kwargs['PATH'], 'POSYDON_data')
            if not os.path.isdir(data_path):
                os.makedirs(data_path)

                dirs = []
                grid_dirs = ['HMS-HMS', 'CO-HMS_RLO', 'CO-HeMS', 'single_HMS',
                            'single_HeMS']
                interp_dirs = ['interpolators', 'interpolators/1NN_1NN',
                               'interpolators/linear3c_kNN']
                for name1 in grid_dirs:
                    dirs.append(os.path.join(data_path,name1))
                    for name2 in interp_dirs:
                        dirs.append(os.path.join(data_path,name1,name2))

                for dir_ in  dirs:
                    if not os.path.isdir(dir_):
                        os.makedirs(dir_)

def slurm_job(job_name,
              PATH_TO_GRIDS=None,
              PATH_TO_POSYDON=None,
              PATH=None,
              ACCOUNT=None,
              PARTITION=None,
              WALLTIME=None,
              MAILTYPE=None,
              EMAIL=None,
              CREATE_GRID_SLICES=False,
              COMBINE_GRID_SLICES=False,
              PLOT_GRIDS=False,
              CHECK_FAILURE_RATE=False,
              POST_PROCESSING=False,
              TRAIN_INTERPOLATORS=False,
              EXPORT_DATASET=False,
              RERUN=False,
              STOP_BEFORE_CARBON_DEPLETION=1,
              RERUN_TYPE='',
              verbose=False,
              **kwargs):

    path_to_csv_file = os.path.join(PATH,f"{job_name}.csv")

    with open(f'{job_name}.slurm', 'w') as f:
        if ('step' in job_name) and (len(job_name.split("_")) > 1):
            STEPID = job_name.split("_")[1]
        elif job_name == 'rerun':
            STEPID = 'R'
        else:
            STEPID = ''
        f.write("#!/bin/bash\n")
        f.write(f"#SBATCH --account={ACCOUNT}\n")
        f.write(f"#SBATCH --partition={PARTITION}\n")
        f.write("#SBATCH -N 1\n")
        f.write("#SBATCH --cpus-per-task 1\n")
        f.write("#SBATCH --ntasks-per-node 1\n")
        f.write(f"#SBATCH --time={WALLTIME}\n")
        f.write(f"#SBATCH --job-name=psygrid{STEPID}\n")
        f.write("#SBATCH --mem-per-cpu=4G\n")

        if EMAIL is not None:
            f.write(f"#SBATCH --mail-type={MAILTYPE}\n")
            f.write(f"#SBATCH --mail-user={EMAIL}\n")

        if job_name in ['step_1', 'step_3', 'step_4','step_5', 'step_6',
                        'step_7', 'rerun']:
            df = pd.read_csv(path_to_csv_file)
            N = df.shape[0]-1
        elif job_name in ['step_2']:
            df = pd.read_csv(path_to_csv_file)
            N = df.shape[1]-1
        else:
            raise ValueError('This should never happen!')
        if N<0:
            raise ValueError(f'{job_name} has no jobs to run, please check '
                             f'{path_to_csv_file}')
        f.write(f"#SBATCH --array=0-{N}\n")
        slurm_array = '$SLURM_ARRAY_TASK_ID'

        if job_name == 'step_1':
            f.write("#SBATCH --open-mode=truncate\n")
            f.write(f"#SBATCH --output={PATH}/logs/grid_slice_%a.out\n")

        if job_name == 'step_2':
            path_to_out_file = os.path.join(PATH, 'logs',
                                            'combine_grid_slices.out')
            if os.path.exists(path_to_out_file): # move old data to a copy
                path_to_old_out_file = os.path.join(PATH, 'logs',
                                                 'combine_grid_slices.old.out')
                if os.path.exists(path_to_old_out_file): # delete old copy
                    os.remove(path_to_old_out_file)
                shutil.move(path_to_out_file, path_to_old_out_file)
            f.write("#SBATCH --open-mode=append\n")
            f.write(f"#SBATCH --output={path_to_out_file}\n")

        if job_name == 'step_3':
            path_to_out_file = os.path.join(PATH, 'logs', 'plot_grid.out')
            if os.path.exists(path_to_out_file): # move old data to a copy
                path_to_old_out_file = os.path.join(PATH, 'logs',
                                                    'plot_grid.old.out')
                if os.path.exists(path_to_old_out_file): # delete old copy
                    os.remove(path_to_old_out_file)
                shutil.move(path_to_out_file, path_to_old_out_file)
            f.write("#SBATCH --open-mode=append\n")
            f.write(f"#SBATCH --output={path_to_out_file}\n")
            f.write("unset DISPLAY\n")

        if job_name == 'step_4':
            path_to_out_file = os.path.join(PATH, 'logs',
                                            'check_failure_rate.out')
            if os.path.exists(path_to_out_file): # move old data to a copy
                path_to_old_out_file = os.path.join(PATH, 'logs',
                                                  'check_failure_rate.old.out')
                if os.path.exists(path_to_old_out_file): # delete old copy
                    os.remove(path_to_old_out_file)
                shutil.move(path_to_out_file, path_to_old_out_file)
            f.write("#SBATCH --open-mode=append\n")
            f.write(f"#SBATCH --output={path_to_out_file}\n")

        if job_name == 'step_5':
            f.write("#SBATCH --open-mode=truncate\n")
            f.write(f"#SBATCH --output={PATH}/logs/post_processing_%a.out\n")
            f.write(f"export PATH_TO_POSYDON={PATH_TO_POSYDON}\n")

        if job_name == 'step_6':
            f.write("#SBATCH --open-mode=truncate\n")
            f.write(f"#SBATCH --output={PATH}/logs/train_interpolators_%a.out\n")

        if job_name == 'step_7':
            path_to_out_file = os.path.join(PATH, 'logs',
                                            'export_dataset.out')
            if os.path.exists(path_to_out_file): # move old data to a copy
                path_to_old_out_file = os.path.join(PATH, 'logs',
                                                    'export_dataset.old.out')
                if os.path.exists(path_to_old_out_file): # delete old copy
                    os.remove(path_to_old_out_file)
                shutil.move(path_to_out_file, path_to_old_out_file)
            f.write("#SBATCH --open-mode=append\n")
            f.write(f"#SBATCH --output={path_to_out_file}\n")

        if job_name == 'rerun':
            path_to_out_file = os.path.join(PATH, 'logs', 'rerun.out')
            if os.path.exists(path_to_out_file): # move old data to a copy
                path_to_old_out_file = os.path.join(PATH, 'logs',
                                                    'rerun.old.out')
                if os.path.exists(path_to_old_out_file): # delete old copy
                    os.remove(path_to_old_out_file)
                shutil.move(path_to_out_file, path_to_old_out_file)
            f.write("#SBATCH --open-mode=append\n")
            f.write(f"#SBATCH --output={path_to_out_file}\n")

        f.write(f"\nsrun python {PATH_TO_POSYDON}/bin/run-pipeline {PATH_TO_GRIDS} {path_to_csv_file} {slurm_array} {STOP_BEFORE_CARBON_DEPLETION} {RERUN_TYPE}")

# create csv file with a list of all grid paths to process
def create_csv(GRID_TYPES, METALLICITIES, GRID_SLICES, COMPRESSIONS,
               step_name=None, VERSION=None, GRIDS_COMBINED=None,
               INTERPOLATION_METHODS=None, RERUN_TYPE='',
               DROP_MISSING_FILES=False, PATH_TO_GRIDS=None,
               PATH=None, previously_created_files=[], **kwargs):

    # number of grid types
    N = len(GRID_TYPES)
    if ( N != len(METALLICITIES) or N != len(GRID_SLICES) or
         N != len(COMPRESSIONS)):
        raise ValueError('Missmatch between the len of GRID_TYPES, '
                         'METALLICITIES, GRID_SLICES, COMPRESSIONS.')

    grids = []
    grids_compression = []
    plot_dirs = []
    processed_grids = []
    interpolators = []
    export_path = []
    rerun_path = []
    newly_created_files = []
    df = pd.DataFrame()
    for l, grid_type in enumerate(GRID_TYPES):

        METALLICITIES_ = METALLICITIES[l]
        GRID_SLICES_ = GRID_SLICES[l]
        COMPRESSIONS_ = COMPRESSIONS[l]

        for metallicity in METALLICITIES_:
            for i, grid_slice in enumerate(GRID_SLICES_):
                for compression in COMPRESSIONS_:
                    if step_name == 'step_1':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, VERSION, metallicity, grid_slice))
                        grids_compression.extend([compression])
                    elif step_name == 'step_2':
                        if N != len(GRIDS_COMBINED):
                            raise ValueError('len(GRID_TYPES) != len(GRIDS_COMBINED)!')
                        if len(GRID_SLICES[l]) != len(GRIDS_COMBINED[l]):
                            raise ValueError('len(GRID_SLICES[l]) != len(GRIDS_COMBINED[l])!')
                        combine_grid_slices = []
                        # grid_slice is a batch
                        for grid_slice_ in grid_slice:
                                path_to_grid = os.path.join(PATH_TO_GRIDS,
                                              grid_type, VERSION, metallicity,
                                              compression, grid_slice_+'.h5')
                                combine_grid_slices.append(path_to_grid)
                        path_to_grid_combined = os.path.join(PATH_TO_GRIDS,
                                                             grid_type, VERSION,
                                                             metallicity,
                                                             compression,
                                                             GRIDS_COMBINED[l][i]+'.h5')
                        df_tmp = pd.DataFrame()
                        df_tmp[path_to_grid_combined] = combine_grid_slices
                        df = pd.concat([df,df_tmp], axis=1)
                    elif step_name == 'step_3':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, VERSION, metallicity,
                                      compression, grid_slice+'.h5'))
                        if 'RLO' in compression:
                            plot_dirs.append(os.path.join(PATH_TO_GRIDS,
                                             grid_type, VERSION, metallicity,
                                             'plots', 'RLO_'+grid_slice))
                        else:
                            plot_dirs.append(os.path.join(PATH_TO_GRIDS,
                                             grid_type, VERSION, metallicity,
                                             'plots', grid_slice))
                    elif step_name == 'step_4':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, VERSION, metallicity,
                                      compression, grid_slice+'.h5'))
                    elif step_name == 'step_5':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, VERSION, metallicity,
                                      compression, grid_slice+'.h5'))
                        processed_grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, VERSION, metallicity,
                                      compression, grid_slice+'_processed.h5'))
                    elif step_name == 'step_6':
                        for method in INTERPOLATION_METHODS:
                            grids.append(os.path.join(PATH_TO_GRIDS,
                                          grid_type, VERSION, metallicity,
                                          compression, grid_slice+'.h5'))
                            if 'RLO' in compression:
                                interpolators.append(os.path.join(PATH_TO_GRIDS,
                                                grid_type, VERSION, metallicity,
                                                'interpolation_objects',
                                                'IF_'+method+'_RLO.pkl'))
                            else:
                                interpolators.append(os.path.join(PATH_TO_GRIDS,
                                                grid_type, VERSION, metallicity,
                                                'interpolation_objects',
                                                'IF_'+method+'.pkl'))
                    elif step_name == 'step_7':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, VERSION, metallicity,
                                      compression, grid_slice+'.h5'))
                        if 'RLO' in compression:
                            export_path.append(os.path.join(PATH, 'POSYDON_data',
                                                            grid_type+'_RLO',
                                                            metallicity+'.h5'))
                        else:
                            export_path.append(os.path.join(PATH, 'POSYDON_data',
                                                            grid_type,
                                                            metallicity+'.h5'))
                        for method in [['linear','linear3c_kNN'],
                                        ['1NN','1NN_1NN']]:
                            if 'RLO' in compression:
                                grids.append(os.path.join(PATH_TO_GRIDS,
                                          grid_type, VERSION, metallicity,
                                          'interpolation_objects',
                                          'IF_'+method[0]+'_RLO.pkl'))
                                export_path.append(os.path.join(PATH,
                                                            'POSYDON_data',
                                                            grid_type+'_RLO',
                                                            'interpolators',
                                                            method[1],
                                                            metallicity+'.pkl'))
                            else:
                                grids.append(os.path.join(PATH_TO_GRIDS,
                                          grid_type, VERSION, metallicity,
                                          'interpolation_objects',
                                          'IF_'+method[0]+'.pkl'))
                                export_path.append(os.path.join(PATH,
                                                            'POSYDON_data',
                                                            grid_type,
                                                            'interpolators',
                                                            method[1],
                                                            metallicity+'.pkl'))
                    elif step_name == 'rerun':
                        grids.append(os.path.join(PATH_TO_GRIDS,
                                      grid_type, VERSION, metallicity,
                                      compression, grid_slice+'.h5'))
                        rerun_path.append(os.path.join(PATH_TO_GRIDS, grid_type,
                                     VERSION, metallicity,
                                     'rerun_'+RERUN_TYPE+'_'+grid_slice))

    # saving dataset to csv file
    if step_name != 'step_2':
        grids = np.array(grids)
        df['path_to_grid'] = grids
        if step_name == 'step_1':
            df['compression'] = grids_compression
        elif step_name == 'step_3':
            df['path_to_plot'] = plot_dirs
        elif step_name == 'step_5':
            df['path_to_processed_grid'] = processed_grids
        elif step_name == 'step_6':
            df['path_to_interpolator'] = interpolators
        elif step_name == 'step_7':
            df['export_path'] = export_path
        elif step_name == 'rerun':
            df['rerun_path'] = rerun_path
        # drop lines when grid directories are not found
        if DROP_MISSING_FILES:
            drop_rows = []
            for row in df.index:
                path = df.at[row,'path_to_grid']
                if (not os.path.exists(path) and
                    path not in previously_created_files):
                    drop_rows.append(row)
            if len(drop_rows)>0:
                print('')
                print(f'----------- {step_name} -----------')
                print('The following grids will not be processed because the '
                      'files/directories are missing! If this warning message '
                      'is unexpected to you, please check the file paths!')
                for row in drop_rows:
                    print(df.at[row,'path_to_grid'])
                print('')
            df = df.drop(index=drop_rows)
        if step_name == 'step_1':
            for row in df.index:
                path = df.at[row,'path_to_grid']
                compression = df.at[row,'compression']
                newly_created_files.append(get_new_grid_name(path,compression))
        elif step_name == 'step_5':
            newly_created_files = df['path_to_processed_grid'].to_list()
        elif step_name == 'step_6':
            newly_created_files = df['path_to_interpolator'].to_list()
        elif step_name == 'step_7':
            newly_created_files = df['export_path'].to_list()
        elif step_name == 'rerun':
            newly_created_files = df['rerun_path'].to_list()
    else:
        # step 2
        # drop columns when psygrid files are not found
        if DROP_MISSING_FILES:
            first_time = True
            drop_columns = []
            for column in df.keys():
                drop_rows = []
                for row in df.index:
                    path = df.at[row,column]
                    if pd.notna(path):
                        if (not os.path.exists(path) and
                            path not in previously_created_files):
                            drop_rows.append(row)
                if len(drop_rows)>0:
                    if first_time:
                        print('')
                        print(f'----------- {step_name} -----------')
                        print('If this warning message is unexpected to you, '
                              'please check that step 1 occoured '
                              'succesffully!')
                        first_time = False
                    print(f'In {column} the following grids are skipped!')
                    for row in drop_rows:
                        print(df.at[row,column])
                        df.at[row,column] = np.NaN
                    print('')
                    newcol = df[column].dropna().to_list()
                    if len(newcol)==0:
                        drop_columns.append(column)
#                        print(f'No grids left drop {column}')
#                        print('')
                    newcol.extend((df.shape[0]-len(newcol))*[np.nan])
                    df[column] = newcol
            if len(drop_columns)>0:
                if first_time:
                    print('')
                    print(f'----------- {step_name} -----------')
                    print('If this warning message is unexpected to you, '
                          'please check that step 1 occoured '
                          'succesffully!')
                    first_time = False
                print('The following grids will not be combined as all grid '
                      'slices are missing!')
                for column in drop_columns:
                    print(column)
                print('')
#            df = df.drop(columns=drop_columns)
            df = df.dropna(axis=0, how='all').dropna(axis=1, how='all')
        newly_created_files = df.keys().to_list()

    output_fname = f'{step_name}.csv'
    df.to_csv(os.path.join(PATH, output_fname), index=False)
    
    return newly_created_files

if __name__ == '__main__':

    if len(sys.argv) >= 2:
        ini_file_path = str(sys.argv[1])
        if '.' not in ini_file_path:
            ini_file_path = './' + ini_file_path

    pipeline = PostProcessingPipeline(ini_file_path)
    pipeline.create_csv_and_slurm_job_files()
    pipeline.create_logs_dir()
    pipeline.create_export_dir()

    # TODO:
    # - add a linked slurm job arrays which are waiting for each others
    # - DROP_MISSING_FILES should be an imput of each step
